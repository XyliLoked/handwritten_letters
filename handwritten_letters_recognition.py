"""–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –†–£–ö–û–ü–ò–°–ù–´–• –ê–ù–ì–õ–ò–ô–°–ö–ò–• –ë–£–ö–í –° –£–°–ö–û–†–ï–ù–ò–ï–ú"""

# ==================== –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö ====================
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 12
print("‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

# ==================== –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ====================
print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º dataset —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö –±—É–∫–≤...")

dataset = np.loadtxt('https://storage.yandexcloud.net/academy.ai/A_Z_Handwritten_Data.csv', delimiter=',')

X = dataset[:,1:785]
Y = dataset[:,0]

print(f"üìä –†–∞–∑–º–µ—Ä dataset: {dataset.shape}")
print(f"üìä –†–∞–∑–º–µ—Ä X (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è): {X.shape}")
print(f"üìä –†–∞–∑–º–µ—Ä Y (–º–µ—Ç–∫–∏): {Y.shape}")

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ü–∏—Ñ—Ä –≤ –±—É–∫–≤—ã
word_dict = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X',24:'Y',25:'Z'}

# ==================== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• ====================
print("\nüëÄ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö –±—É–∫–≤...")

plt.figure(figsize=(15, 10))
for i in range(40):
    x = X[i]
    x = x.reshape((28, 28))
    plt.subplot(5, 8, i+1)
    plt.imshow(x, cmap='gray')
    plt.title(f'{word_dict.get(Y[i])} ({int(Y[i])})')
    plt.axis('off')
plt.tight_layout()
plt.show()

# ==================== –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• ====================
print("\nüîÑ –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/test...")

(x_train, x_test, y_train, y_test) = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=42)

print(f"‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
print(f"   x_train: {x_train.shape}")
print(f"   y_train: {y_train.shape}")
print(f"   x_test: {x_test.shape}")
print(f"   y_test: {y_test.shape}")

# ==================== –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ====================
print("\nüîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–∏–∫—Å–µ–ª–µ–π (0-255 -> 0-1)
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# One-Hot Encoding –¥–ª—è 26 –∫–ª–∞—Å—Å–æ–≤ –±—É–∫–≤
y_train_categorical = tf.keras.utils.to_categorical(y_train, 26)
y_test_categorical = tf.keras.utils.to_categorical(y_test, 26)

print("‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")

# ==================== –°–û–ó–î–ê–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ú–û–î–ï–õ–ò ====================
def create_optimized_model():
    """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º"""
    model = models.Sequential([
        # –ü–µ—Ä–≤—ã–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        layers.Dense(1024, activation='relu', input_shape=(784,)),
        layers.Dropout(0.3),

        # –í—Ç–æ—Ä–æ–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.3),

        # –¢—Ä–µ—Ç–∏–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.2),

        # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),

        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (26 –Ω–µ–π—Ä–æ–Ω–æ–≤ –¥–ª—è 26 –±—É–∫–≤)
        layers.Dense(26, activation='softmax')
    ])

    # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ö–û–ú–ü–ò–õ–Ø–¶–ò–Ø –° –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ú–ò –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†–ê–ú–ò
    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            learning_rate=0.001,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07
        ),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

print("üß† –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º Adam...")
model = create_optimized_model()

print("\nüìê –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
model.summary()

# ==================== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –£–°–ö–û–†–ï–ù–ò–ï–ú ====================
print("\nüéØ –ù–∞—á–∏–Ω–∞–µ–º –£–°–ö–û–†–ï–ù–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ (5 —ç–ø–æ—Ö)...")

# –£–õ–£–ß–®–ï–ù–ù–´–ï –ö–û–õ–ë–≠–ö–ò –î–õ–Ø –£–°–ö–û–†–ï–ù–ò–Ø
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=3,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    restore_best_weights=True,
    mode='max',
    verbose=1
)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=2,  # –ë–æ–ª–µ–µ –±—ã—Å—Ç—Ä–∞—è —Ä–µ–∞–∫—Ü–∏—è
    min_lr=0.0001,
    verbose=1
)

print("‚è±Ô∏è  –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 5 —ç–ø–æ—Ö–∞—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º Adam...")

history = model.fit(
    x_train,
    y_train_categorical,
    epochs=5,  # –¢–û–õ–¨–ö–û 5 –≠–ü–û–•
    batch_size=256,
    validation_data=(x_test, y_test_categorical),
    callbacks=[early_stopping, reduce_lr],  # ‚¨ÖÔ∏è –í–û–¢ –¢–£–¢ –£–ë–†–ê–õ–ò tensorboard
    verbose=1
)

print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

# ==================== –°–†–ê–í–ù–ï–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†–û–í ====================
print("\nüî¨ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")

optimizers = {
    'Adam': tf.keras.optimizers.Adam(learning_rate=0.001),
    'RMSprop': tf.keras.optimizers.RMSprop(learning_rate=0.001),
    'Nadam': tf.keras.optimizers.Nadam(learning_rate=0.001),
    'Adamax': tf.keras.optimizers.Adamax(learning_rate=0.001)
}

optimizer_results = {}

for opt_name, optimizer in optimizers.items():
    print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º {opt_name}...")

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ç–µ–∫—É—â–∏–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º
    test_model = models.Sequential([
        layers.Dense(512, activation='relu', input_shape=(784,)),
        layers.Dropout(0.3),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(26, activation='softmax')
    ])

    test_model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 3 —ç–ø–æ—Ö–∞—Ö
    test_model.fit(
        x_train, y_train_categorical,
        epochs=3,
        batch_size=256,
        verbose=0
    )

    # –û—Ü–µ–Ω–∫–∞
    test_loss, test_accuracy = test_model.evaluate(x_test, y_test_categorical, verbose=0)
    optimizer_results[opt_name] = test_accuracy
    print(f"   {opt_name} —Ç–æ—á–Ω–æ—Å—Ç—å: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
plt.figure(figsize=(10, 6))
names = list(optimizer_results.keys())
accuracies = [optimizer_results[name] * 100 for name in names]

bars = plt.bar(names, accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ (3 —ç–ø–æ—Ö–∏)', fontsize=14)
plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
plt.ylim(80, 95)
plt.grid(True, alpha=0.3)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,
            f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

# ==================== –ê–ù–ê–õ–ò–ó –ü–†–û–¶–ï–°–°–ê –û–ë–£–ß–ï–ù–ò–Ø ====================
print("\nüìà –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è...")

def plot_training_history(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    ax1.plot(history.history['loss'], label='–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏', linewidth=2, marker='o')
    ax1.plot(history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏', linewidth=2, marker='s')
    ax1.set_title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (5 —ç–ø–æ—Ö)')
    ax1.set_xlabel('–≠–ø–æ—Ö–∞')
    ax1.set_ylabel('–ü–æ—Ç–µ—Ä–∏')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    ax2.plot(history.history['accuracy'], label='–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å', linewidth=2, marker='o')
    ax2.plot(history.history['val_accuracy'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å', linewidth=2, marker='s')
    ax2.set_title('–¢–æ—á–Ω–æ—Å—Ç—å (5 —ç–ø–æ—Ö)')
    ax2.set_xlabel('–≠–ø–æ—Ö–∞')
    ax2.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

plot_training_history(history)

# ==================== –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò ====================
print("\nüß™ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

test_loss, test_accuracy = model.evaluate(x_test, y_test_categorical, verbose=0)

print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•:")
print(f"‚úÖ –ü–æ—Ç–µ—Ä–∏ (Loss): {test_loss:.4f}")
print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred_proba = model.predict(x_test, verbose=0)
y_pred = np.argmax(y_pred_proba, axis=1)

# ==================== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ====================
print("\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

def detailed_analysis(y_true, y_pred, word_dict):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(16, 14))

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    plt.subplot(2, 2, 1)
    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',
                xticklabels=[word_dict[i] for i in range(26)],
                yticklabels=[word_dict[i] for i in range(26)])
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫', fontsize=14)
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –±—É–∫–≤—ã')
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –±—É–∫–≤—ã')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)

    # –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
    plt.subplot(2, 2, 2)
    accuracy_per_class = []
    for i in range(26):
        mask = y_true == i
        if mask.sum() > 0:
            accuracy = (y_pred[mask] == i).mean()
            accuracy_per_class.append(accuracy)

    plt.bar(range(26), accuracy_per_class, color='green', alpha=0.7)
    plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –±—É–∫–≤–∞–º', fontsize=14)
    plt.xlabel('–ë—É–∫–≤–∞')
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.xticks(range(26), [word_dict[i] for i in range(26)], rotation=45)
    plt.ylim(0.7, 1.0)
    plt.grid(True, alpha=0.3)

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
    plt.subplot(2, 2, 3)
    errors = y_pred != y_true
    error_distribution = [((y_true[errors] == i).sum()) for i in range(26)]

    plt.bar(range(26), error_distribution, color='red', alpha=0.7)
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø–æ –±—É–∫–≤–∞–º', fontsize=14)
    plt.xlabel('–ë—É–∫–≤–∞')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫')
    plt.xticks(range(26), [word_dict[i] for i in range(26)], rotation=45)
    plt.grid(True, alpha=0.3)

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.subplot(2, 2, 4)
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]

    categories = ['–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è', '–¢–µ—Å—Ç–æ–≤–∞—è']
    accuracies = [final_train_acc, final_val_acc]
    colors = ['blue', 'orange']

    bars = plt.bar(categories, accuracies, color=colors, alpha=0.7)
    plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ (5 —ç–ø–æ—Ö)', fontsize=14)
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.ylim(0.8, 1.0)

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, acc in zip(bars, accuracies):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.show()

    # –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    print("üìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
    print(classification_report(y_test, y_pred,
                            target_names=[word_dict[i] for i in range(26)], digits=3))

detailed_analysis(y_test, y_pred, word_dict)

# ==================== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô ====================
print("\nüëÅÔ∏è –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")

def visualize_predictions(x_original, y_true, y_pred, word_dict, num_examples=12):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""

    correct_indices = np.where(y_pred == y_true)[0]
    wrong_indices = np.where(y_pred != y_true)[0]

    print(f"‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(correct_indices)} ({len(correct_indices)/len(y_true)*100:.2f}%)")
    print(f"‚ùå –û—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(wrong_indices)} ({len(wrong_indices)/len(y_true)*100:.2f}%)")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    if len(correct_indices) > 0:
        print("\n‚úÖ –ü–†–ò–ú–ï–†–´ –ü–†–ê–í–ò–õ–¨–ù–´–• –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
        correct_samples = np.random.choice(correct_indices, min(num_examples, len(correct_indices)), replace=False)

        fig, axes = plt.subplots(3, 4, figsize=(15, 12))
        axes = axes.ravel()

        for i, idx in enumerate(correct_samples):
            img = x_original[idx].reshape(28, 28)
            axes[i].imshow(img, cmap='gray')
            confidence = np.max(model.predict(x_test[idx:idx+1], verbose=0))
            true_letter = word_dict[y_true[idx]]
            pred_letter = word_dict[y_pred[idx]]

            axes[i].set_title(f'True: {true_letter}, Pred: {pred_letter}\nConf: {confidence:.3f}',
                            color='green', fontweight='bold')
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    if len(wrong_indices) > 0:
        print("\n‚ùå –ü–†–ò–ú–ï–†–´ –û–®–ò–ë–û–ß–ù–´–• –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
        wrong_samples = np.random.choice(wrong_indices, min(num_examples, len(wrong_indices)), replace=False)

        fig, axes = plt.subplots(3, 4, figsize=(15, 12))
        axes = axes.ravel()

        for i, idx in enumerate(wrong_samples):
            img = x_original[idx].reshape(28, 28)
            axes[i].imshow(img, cmap='gray')
            confidence = np.max(model.predict(x_test[idx:idx+1], verbose=0))
            true_letter = word_dict[y_true[idx]]
            pred_letter = word_dict[y_pred[idx]]

            axes[i].set_title(f'True: {true_letter}, Pred: {pred_letter}\nConf: {confidence:.3f}',
                            color='red', fontweight='bold')
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

x_test_original = x_test * 255
visualize_predictions(x_test_original, y_test, y_pred, word_dict)

# ==================== –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò –í–´–í–û–î–´ ====================
print("\n" + "="*70)
print("üéâ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ï–ô")
print("="*70)

final_accuracy = test_accuracy * 100
print(f"\nüìä –û–°–ù–û–í–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
print(f"   üéØ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {final_accuracy:.2f}%")
print(f"   ‚è±Ô∏è  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {len(history.history['accuracy'])}")
print(f"   üîß –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")
print(f"   ‚ùå –û—à–∏–±–æ–∫: {(y_pred != y_test).sum()} –∏–∑ {len(y_test)}")

# –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
train_final_acc = history.history['accuracy'][-1] * 100
overfitting_gap = train_final_acc - final_accuracy

print(f"\nüìà –ê–ù–ê–õ–ò–ó –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:")
print(f"   –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {train_final_acc:.2f}%")
print(f"   –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.2f}%")
print(f"   –†–∞–∑–Ω–∏—Ü–∞: {overfitting_gap:.2f}%")

# –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
initial_acc = history.history['val_accuracy'][0] * 100
final_acc = history.history['val_accuracy'][-1] * 100
improvement = final_acc - initial_acc

print(f"   –ù–∞—á–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {initial_acc:.2f}%")
print(f"   –ö–æ–Ω–µ—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_acc:.2f}%")
print(f"   –£–ª—É—á—à–µ–Ω–∏–µ –∑–∞ 5 —ç–ø–æ—Ö: {improvement:.2f}%")

if improvement > 15:
    print("   üöÄ –û—Ç–ª–∏—á–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è!")
elif improvement > 10:
    print("   ‚ö° –•–æ—Ä–æ—à–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è")
else:
    print("   üìâ –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å")

print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
model.save('alphabet_recognition_optimized.h5')
print("‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")